{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f5aed8c1",
   "metadata": {},
   "source": [
    "# Topic Modeling on Short Text with BERTopic and BERTweet\n",
    "\n",
    "### Team members: Emily Altland, Terryl Dodson, Maheep Mahat, Daniel Manesh, Kiet Nguyen, Tianjiao Yu\n",
    "\n",
    "Semester: Spring 2022\n",
    "\n",
    "Instructor: Dr. Dawei Zhou"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fbeaf1d",
   "metadata": {},
   "source": [
    "First, we import the necessary packages to run our code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3aae4b5c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mD:\\AppData\\Local\\Temp/ipykernel_30004/1696982383.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;31m# BERTopic (our modification of the source code)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m# Dimension reduction\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\CS5824\\aml_vts22\\bertopic\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mbertopic\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_bertopic\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mBERTopic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0m__version__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"0.9.4\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m __all__ = [\n",
      "\u001b[1;32mD:\\CS5824\\aml_vts22\\bertopic\\_bertopic.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFutureWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfilterwarnings\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"ignore\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcategory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mUserWarning\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'yaml'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import torch\n",
    "\n",
    "# BERTopic (our modification of the source code)\n",
    "from bertopic._bertopic import BERTopic\n",
    "\n",
    "# Dimension reduction\n",
    "from umap import UMAP\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "# Embeddings\n",
    "from flair.embeddings import TransformerDocumentEmbeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Evaluation\n",
    "import gensim.corpora as corpora\n",
    "from gensim.models import CoherenceModel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d91e7aa",
   "metadata": {},
   "source": [
    "Here is our function to train our BERTopic model. We pass to it our data in the form of a Pandas Series, an embedding model (in our case, it is either Sentence Transformers or BERTweet), and a dimension reduction model (UMAP, PCA, or t-SNE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84535bed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_bertopic(data, embedding_model, dimension_reduction_model):\n",
    "    vectorizer_model = CountVectorizer(ngram_range=(1, 1), min_df=1)\n",
    "    if isinstance(embedding_model, TSNE):\n",
    "        nr_topics = 5\n",
    "    else:\n",
    "        nr_topics = \"auto\"\n",
    "    topic_model = BERTopic(\n",
    "        embedding_model=embedding_model,\n",
    "        nr_topics=nr_topics,\n",
    "        top_n_words=20,\n",
    "        min_topic_size=30,\n",
    "        verbose=True,\n",
    "        low_memory=True,\n",
    "        vectorizer_model=vectorizer_model,\n",
    "        umap_model=dimension_reduction_model,\n",
    "    )\n",
    "    topics, _ = topic_model.fit_transform(data.tolist())\n",
    "    return topic_model, topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c61e958",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coherence_score(data, topic_model, topics, coherence):\n",
    "    # Extract vectorizer and tokenizer from BERTopic\n",
    "    vectorizer = topic_model.vectorizer_model\n",
    "    tokenizer = vectorizer.build_tokenizer()\n",
    "\n",
    "    # Extract features for Topic Coherence evaluation\n",
    "    tokens = [tokenizer(doc) for doc in data]\n",
    "    # tokens = [token for token in tokens if token!='']\n",
    "\n",
    "    dictionary = corpora.Dictionary(tokens)\n",
    "    corpus = [dictionary.doc2bow(token) for token in tokens]\n",
    "    topic_words = [\n",
    "        [words for words, _ in topic_model.get_topic(topic) if words != \"\"]\n",
    "        for topic in range(len(set(topics)) - 1)\n",
    "    ]\n",
    "\n",
    "    # Evaluate\n",
    "    coherence_model = CoherenceModel(\n",
    "        topics=topic_words,\n",
    "        texts=tokens,\n",
    "        corpus=corpus,\n",
    "        dictionary=dictionary,\n",
    "        coherence=coherence,\n",
    "    )\n",
    "    return coherence_model.get_coherence()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d446c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    data_path = os.path.join(\n",
    "        os.path.dirname(os.path.abspath(__file__)),\n",
    "        \"data\",\n",
    "        \"preprocessed_tweets\",\n",
    "        \"all_tweets.csv\",\n",
    "    )\n",
    "    df = pd.read_csv(data_path, header=0)\n",
    "    df = df.text.dropna()[:20000] # Train on sample of 20,000 tweets\n",
    "\n",
    "    # Train BERTopic using BERTweet base vs. BERT base as our embedding model\n",
    "    bertweet = TransformerDocumentEmbeddings(\"vinai/bertweet-base\")\n",
    "    sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "    embedding_models = [bertweet, sentence_model]\n",
    "    embedding_model_names = [\"BERTweet\", \"BERT base\"]\n",
    "    \n",
    "    # Dimension reduction models\n",
    "    tsne = TSNE(n_components=5, init=\"pca\", method=\"exact\", random_state=2022)\n",
    "    pca = PCA(n_components=5, random_state=2022)\n",
    "    umap = UMAP(n_neighbors=35, n_components=5, min_dist=0.0, metric=\"euclidean\", random_state=2022)\n",
    "\n",
    "    dimension_reduction_models = [tsne, pca, umap]\n",
    "    dimension_reduction_model_names = [\"t-SNE\", \"PCA\", \"UMAP\"]\n",
    "\n",
    "    for embedding_model, ename in zip(embedding_models, embedding_model_names):\n",
    "        for dimension_reduction_model, dname in zip(dimension_reduction_models, dimension_reduction_model_names):\n",
    "            print(f\"{datetime.now().strftime('%H:%M:%S')}: Computing coherence score: {ename} with {dname}.\")\n",
    "            topic_model, topics = train_bertopic(df, embedding_model, dimension_reduction_model)\n",
    "            score = get_coherence_score(df, topic_model, topics, \"u_mass\")\n",
    "            print(\n",
    "                f\"{datetime.now().strftime('%H:%M:%S')}: {ename} with {dname} UMass score: {score}.\"\n",
    "            )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
